import torch
import numpy as np
import librosa
import yaml
import os
from model import RawNet 

# --- C·∫§U H√åNH ---

# 1. ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model .pth 
MODEL_PATH = "models/model_LA_weighted_CCE_100_32_0.0001/epoch_15.pth"

# 2. ƒê∆∞·ªùng d·∫´n ƒë·∫øn file c·∫•u h√¨nh YAML 
YAML_CONFIG = 'model_config_RawNet.yaml'

# 3. ƒê∆∞·ªùng d·∫´n file √¢m thanh c·∫ßn ki·ªÉm tra
AUDIO_FILE = 'test_audio.mp3' 

# 4. C·∫•u h√¨nh thi·∫øt b·ªã
device = 'cuda' if torch.cuda.is_available() else 'cpu'

def load_model(model_path, config_path):
    # B∆∞·ªõc 1: ƒê·ªçc c·∫•u h√¨nh t·ª´ file YAML (ƒë·ªÉ kh·ªõp size v·ªõi model ƒë√£ train)
    if not os.path.exists(config_path):
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file config t·∫°i {config_path}")
        print("H√£y ƒë·∫£m b·∫£o file 'model_config_RawNet.yaml' n·∫±m c√πng th∆∞ m·ª•c v·ªõi test.py")
        exit()
        
    with open(config_path, 'r') as f_yaml:
        parser1 = yaml.safe_load(f_yaml)
        # L·∫•y tham s·ªë model t·ª´ key 'model' trong file yaml
        d_args = parser1['model'] 
    
    print(f"üîπ C·∫•u h√¨nh model loaded: first_conv={d_args['first_conv']}")

    # B∆∞·ªõc 2: Kh·ªüi t·∫°o model v·ªõi c·∫•u h√¨nh v·ª´a ƒë·ªçc
    model = RawNet(d_args, device).to(device)
    
    # B∆∞·ªõc 3: Load tr·ªçng s·ªë
    if not os.path.exists(model_path):
        print(f"Kh√¥ng t√¨m th·∫•y file model: {model_path}")
        exit()

    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"ƒê√£ load model th√†nh c√¥ng: {model_path}")
    except Exception as e:
        print(f"L·ªói load tr·ª±c ti·∫øp: {e}")
        # Th·ª≠ load theo ki·ªÉu checkpoint n·∫øu file pth ch·ª©a c·∫£ optimizer
        checkpoint = torch.load(model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
             model.load_state_dict(checkpoint['model_state_dict'])
             print("ƒê√£ load model t·ª´ checkpoint th√†nh c√¥ng.")
        else:
            print("Kh√¥ng th·ªÉ load model (V·∫´n b·ªã l·ªách size ho·∫∑c sai file).")
            exit()
            
    model.eval() 
    return model

def process_audio(file_path):
    # C·∫Øt ho·∫∑c ƒë·ªám file √¢m thanh cho ƒë·ªß ƒë·ªô d√†i chu·∫©n
    cut = 64600
    try:
        X, fs = librosa.load(file_path, sr=16000)
    except Exception as e:
        print(f"L·ªói th∆∞ vi·ªán Librosa kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file: {e}")
        return None

    X_pad = np.zeros(cut)
    if X.shape[0] < cut:
        X_pad[:X.shape[0]] = X
    else:
        X_pad = X[:cut]
    
    # Chuy·ªÉn th√†nh Tensor
    x_inp = torch.Tensor(X_pad).unsqueeze(0).to(device)
    return x_inp

def predict(model, audio_path):
    tensor_audio = process_audio(audio_path)
    if tensor_audio is None:
        return

    with torch.no_grad():
        output = model(tensor_audio)
        probs = torch.nn.functional.softmax(output, dim=1)
        spoof_score = probs[0][0].item() * 100
        bonafide_score = probs[0][1].item() * 100
        
        print(f"\n--- K·∫æT QU·∫¢ KI·ªÇM TRA: {audio_path} ---")
        print(f"Gi·∫£ m·∫°o (Spoof): {spoof_score:.2f}%")
        print(f"Th·∫≠t (Bonafide): {bonafide_score:.2f}%")
        
        if bonafide_score > spoof_score:
            print("=> K·∫æT LU·∫¨N: √ÇM THANH TH·∫¨T")
        else:
            print("=> K·∫æT LU·∫¨N: √ÇM THANH GI·∫¢ M·∫†O")

if __name__ == "__main__":
    # Load model v·ªõi file config
    model = load_model(MODEL_PATH, YAML_CONFIG)
    
    # Ch·∫°y th·ª≠
    if os.path.exists(AUDIO_FILE):
        predict(model, AUDIO_FILE)
    else:
        print(f"Ch∆∞a c√≥ file √¢m thanh m·∫´u: {AUDIO_FILE}")
        print("H√£y ƒë·ªïi bi·∫øn AUDIO_FILE trong code th√†nh ƒë∆∞·ªùng d·∫´n file b·∫°n mu·ªën test.")